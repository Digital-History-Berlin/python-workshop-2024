{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíª [Regul√§re Ausdr√ºcke](https://en.wikipedia.org/wiki/Regular_expression) - ein Kurz√ºberblick\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um Regul√§re Ausdr√ºcke in Python einsetzen zu k√∂nnen, m√ºssen Sie eine spezielle Bibliothek importieren: [re](https://docs.python.org/3/library/re.html). Das tun Sie √ºber den bereits kennengelernten Befehl `import`. Die [verlinkte Dokumentation](https://docs.python.org/3/library/re.html) sollten Sie f√ºr die Bearbeitung der √úbungsaufgaben zu Rate ziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = \"Das Referat ‚ÄûText+Berg ‚Äì 150 Jahre alpinistische Texte: OCR-Fehler, Crowd Correction‚Äú von SIMON CLEMATIDE (Z√ºrich) diskutierte die Voraussetzungen f√ºr erfolgreiches Crowd Sourcing.[20] Im Rahmen des Projekts Text+Berg realisierte das Institut f√ºr Computerlinguistik der Universit√§t Z√ºrich eine Online-Plattform zur Korrektur des OCR-Textes der digitalisierten Jahrb√ºcher des Schweizerischen Alpenklubs (SAC) von 1864 bis 1899.[21] Entscheidend f√ºr die Motivierung von Freiwilligen und damit f√ºr den Erfolg des Vorhabens waren eine sorgf√§ltig programmierte Benutzeroberfl√§che und ein einfacher Workflow. Dazu kamen begleitende Massnahmen, um die Motivation der Teilnehmenden aufrechtzuerhalten. Hierzu geh√∂rten spielerische Elemente und Layout-Massnahmen, die den Teilnehmenden R√ºckmeldungen zum Datenzustand und zur geleisteten Arbeit geben. Ein Vorteil war, dass es sich beim potentiellen Teilnehmerkreis um gut organisierte und am Thema interessierte Vereinsmitglieder mit hoher Sachkenntnis handelte.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier der √úbersicht halber noch einmal der Auszugs aus dem Tagungsbericht [\"Digitale Daten in den Geisteswissenschaften. Interdisziplin√§re Perspektiven f√ºr semantische und strukturelle Analysen\"](https://www.hsozkult.de/conferencereport/id/fdkn-125120) :\n",
    "\n",
    "> Das Referat ‚ÄûText+Berg ‚Äì 150 Jahre alpinistische Texte: OCR-Fehler, Crowd Correction‚Äú von SIMON CLEMATIDE (Z√ºrich) diskutierte die Voraussetzungen f√ºr erfolgreiches Crowd Sourcing.[20] Im Rahmen des Projekts Text+Berg realisierte das Institut f√ºr Computerlinguistik der Universit√§t Z√ºrich eine Online-Plattform zur Korrektur des OCR-Textes der digitalisierten Jahrb√ºcher des Schweizerischen Alpenklubs (SAC) von 1864 bis 1899.[21] Entscheidend f√ºr die Motivierung von Freiwilligen und damit f√ºr den Erfolg des Vorhabens waren eine sorgf√§ltig programmierte Benutzeroberfl√§che und ein einfacher Workflow. Dazu kamen begleitende Massnahmen, um die Motivation der Teilnehmenden aufrechtzuerhalten. Hierzu geh√∂rten spielerische Elemente und Layout-Massnahmen, die den Teilnehmenden R√ºckmeldungen zum Datenzustand und zur geleisteten Arbeit geben. Ein Vorteil war, dass es sich beim potentiellen Teilnehmerkreis um gut organisierte und am Thema interessierte Vereinsmitglieder mit hoher Sachkenntnis handelte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grundlegende Methoden\n",
    "\n",
    "### [re.search()](https://docs.python.org/3/library/re.html#re.search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suche nach dem Vorkommen eines Teilstrings, der dem RegEx \"ente\" entspricht\n",
    "match_one = re.search(r\"ente\", text)\n",
    "print(match_one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match pr√ºfen\n",
    "text[727:731]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: \n",
    "- `Match`-Objekt, wenn Suche erfolgreich war (Angabe der Indexpositionen des Matches)\n",
    "- `None`, wenn kein Match gefunden wurde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [re.findall()](https://docs.python.org/3/library/re.html#re.findall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alle Vorkommen eines Matches ermitteln\n",
    "match_two = re.findall(r\"ein\", text)\n",
    "print(match_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output: Alle √úbereinstimmungen eines RegEx werden im Listenformat zur√ºckgegeben."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gefahr:** \n",
    "- Die Mustersuche lediglich mit Literalen birgt, wie an diesem Beispiel festgestellt werden kann, die Gefahr des *Over Matchings*, das hei√üt, wir erhalten zu viele falsche positive Ergebnisse (*false positives*). Der Ausdruck ist zu unspezifisch und bietet zu viel Spielraum. Es wird beim Matching kein Unterschied gemacht, ob die Zeichenfolge f√ºr das Wort \"ein\" oder einen Teilstring in einem Wort wie \"Vereinsmitglieder\" steht. \n",
    "- L√∂sung: Definition komplexer Strukturen (dazu unten mehr)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel f√ºr regul√§ren Ausdruck, der das Wort \"ein\" matcht\n",
    "match_three = re.findall(r\"\\bein\\b\", text)\n",
    "print(match_three)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel f√ºr regul√§ren Ausdruck, der das Wort \"ein\" oder \"eine\" matcht\n",
    "match_three = re.findall(r\"\\b(ein|eine)\\b\", text)\n",
    "print(match_three)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Flag: [re.IGNORECASE](https://docs.python.org/3/library/re.html#re.IGNORECASE) oder re.I (Kurzform)\n",
    "Mit diesem Flag kann spezifiziert werden, dass die Gro√ü- und Kleinschreibung ignoriert werden soll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel f√ºr regul√§ren Ausdruck, der das Wort \"ein\" oder \"eine\" matcht\n",
    "match_four = re.findall(r\"\\bdas\\b\", text, re.IGNORECASE)\n",
    "print(match_four)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [re.sub()](https://docs.python.org/3/library/re.html#re.sub)\n",
    "\n",
    "- Die Methode `sub(regex, repl, string)` nimmt einen Regul√§ren Ausdruck und sucht nach √úbereinstimmungen im String. Die Matches werden dann durch ein Replace-Argument ersetzt. Optional kann angegeben werden, wie oft der Vorgang des Ersetzens durchzuf√ºhren ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_text = re.sub(r\"\\b(ein|eine)\\b\", \"TAUSCH\", text)\n",
    "print(updated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [re.split()](https://docs.python.org/3/library/re.html#re.split)\n",
    "- Die Methode `re.split(regex, string, maxsplit=0)` bietet die M√∂glichkeit, einen String auf Basis des Vorkommens eines bestimmten Musters aufzusplitten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufteilen des Textes an Stellen, an denen Punkte oder Kommata vorkommen\n",
    "\n",
    "split_text = re.split(r\"\\b(ein|eine)\\b\", text)\n",
    "\n",
    "for each in split_text:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù **Jetzt:** Kleine Aufgaben zum Verst√§ndnis der Zeichenklassen\n",
    "In Regul√§ren Ausdr√ºcken haben zahlreiche Zeichen eine Sonderbedeutung, was die Definition abstrakter Muster erm√∂glicht. Nachfolgend finden Sie einige grundlegende Elemente der Notation aufgelistet.\n",
    "\n",
    "‚è≥ 15 Minuten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zeichenklassen\n",
    "\n",
    "- `\\w` = Alphanumerische Zeichen, inklusive Unterstrich; entspricht `[a-zA-Z0-9_] `\n",
    "- `\\W` = alles au√üer alphanumerische Zeichen; entspricht `[^a-zA-Z0-9_] `\n",
    "\n",
    "- `\\d` = eine Ziffer; entspricht `[0-9]`\n",
    "- `\\D` = alle Zeichen au√üer Ziffern; entspricht `[^0-9]`\n",
    "\n",
    "- `\\s` = ein Whitespace, also Leerzeichen, Tabs, Newlines usw.; entspricht `[\\t\\n\\r\\f\\v]`\n",
    "- `\\S` = alles au√üer Whitespaces; entspricht `[^\\t\\n\\r\\f\\v]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** Formulieren Sie mit den oben aufgelisteten Zeichenklassen einige Regul√§re Ausdr√ºcke wie im nachfolgenden Beispiel, um ein Gef√ºhl daf√ºr zu bekommen, wie sie funktionieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example\n",
    "nonalpha_chars = re.findall(r\"\\W\", text)\n",
    "print(set(nonalpha_chars))                      # set() nutzen, um Duplikate zu entfernen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** Recherchieren Sie in der [Dokumentation](https://docs.python.org/3/library/re.html#regular-expression-syntax) des `re`-Moduls wof√ºr die nachfolgenden Zeichenklassen stehen. Definieren Sie mit ihnen und Literalen einen regul√§ren Ausdruck, der nur das Wort \"um\" matcht:\n",
    "- `\\b` =\n",
    "- `\\B` ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beispiel f√ºr regul√§ren Ausdruck, der das Wort \"ein\" matcht\n",
    "match = re.findall(r\"\\Bum\\B\", text)\n",
    "print(match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aufgabe:** Lesen Sie auf Basis der obigen Notation die Jahreszahlen aus dem Textauszug aus. Nutzen Sie dazu die `findall`-Methode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_option_a = re.findall(r\"\\d\\d\\d\\d\", text) \n",
    "print(years_option_a)\n",
    "\n",
    "years_option_b = re.findall(r\"[0-9][0-9][0-9][0-9]\", text)\n",
    "print(years_option_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAUSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metazeichen \n",
    "wie `[`, `]`, `^`, `\\\\`, `(`, oder `)` helfen dabei, etwas √ºber Zeichen auszudr√ºcken:\n",
    "\n",
    "- `.` = Platzhalter f√ºr ein beliebiges Zeichen au√üer Newlines; zwei Punkte stehen entsprechend f√ºr zwei beliebige Zeichen usw.\n",
    "  `r\".ut\"` kann demnach mit \"hut\", \"tut\", \"gut\" usw. matchen.\n",
    "  \n",
    "- Die eckigen Klammern \"`[`\" und \"`]`\" werden zur Definition einer Zeichenauswahl eingesetzt. \n",
    "  - Bsp. A) `[abc]` matcht \"a\", \"b\" oder \"c\"\n",
    "  - Bsp. B) `[a-c]` k√ºrzt die Auswahl aus Beispiel A ab, wenn nach einer Sequenz gesucht wird, matcht \"a\", \"b\", oder \"c\".\n",
    "  - Bsp. C) Das Caret-Zeichen \"^\" in den Sets `[^abc]` und `[^a-c]` negiert die Angabe im Range, gematcht wird also irgendein Zeichen au√üer \"a\", \"b\" oder \"c\"\n",
    "  - Wichtig: Sonderzeichen verlieren in Sets ihre Bedeutung. `\"[+*]\"` matcht mit den f√ºr sich selbst stehenden Zeichen \"+\" oder \"*\". Zeichenklassen wie `/w` oder `/W` behalten weiterhin ihre Bedeutung in Sets.\n",
    "\n",
    "- ^ = matcht einen Zeilenanfang; im `MULTILINE`-Modus wird direkt nach einem Newline gematcht\n",
    " - *Sonderbedeutung:* Das sogenannte Caret-Zeichen bedeutet, wenn es in einer Zeichenspanne direkt hinter der √∂ffnenden Klammer steht, die Negation der Auswahl. Denken Sie an die Zeichenklassen `\\D` = `[^0-9]` oder `\\W` = `[^a-zA-Z0-9_]`.\n",
    "- $ = matcht das Zeilenende vor einem Newline (oder schlicht das Ende einer Zeichenkette) \n",
    "- `|` = repr√§sentiert eine Alternative, z.B. kann der Regul√§re Ausdruck `r\"e[s|r]\"` sowohl √úbereinstimmungen mit \"er\" als auch mit \"es\" identifizieren\n",
    "- `()` = steht f√ºr eine Gruppierung, das hei√üt, Sie k√∂nnen bestimmte Teilausdr√ºcke zusammenfassen: z.B. \"(Haus|Master)arbeit\"\n",
    "- `\\` Der Backslash kann genutzt werden um die besondere Bedeutung der Metazeichen aufzuheben. In einem Regul√§ren Ausdruck stehen sie dann nur noch f√ºr sich selbst. Bsp.: `r\"\\[[0-9]\\]\"` matcht eine Zahl, die in eckigen Klammern steht, beispielsweise \"[1]\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_gpt_text = \"\"\"In einem sonnigen Dorf lebten 2 Katzen, 5 kleine katzen und 1 \"Gro√ükatze\". \n",
    "Jeden 3. Tag um 14:30 Uhr, begannen die Katzen mit dem gro√üen Abenteuer: eine Expedition \n",
    "ins 'geheimnisvolle Land der 1000 M√§use'. Katze123 war die Anf√ºhrerin, bekannt f√ºr ihre \n",
    "au√üergew√∂hnlichen F√§higkeiten im 'M√§usefang'. Ihre Devise lautete: 'Auf jede Katze wartet \n",
    "eine Maus, ob gro√ü oder klein, in jedem Haus!' Am Ende des Tages, z√§hlten sie ihre Beute: \n",
    "45 M√§use, 30 K√§sest√ºcke und unz√§hlige Geschichten, die von Generation zu Generation weitererz√§hlt \n",
    "wurden. Es war 1 Miau-fekt Tag!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Platzhalter f√ºr beliebiges Zeichen\n",
    "\n",
    "re.findall(r\"\\bei..\\b\", chat_gpt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suche nach Vorkommen von Katzen oder katzen\n",
    "\n",
    "re.findall(r\"\\b[Kk]atzen\\b\", chat_gpt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suche nach den W√∂rtern M√§use oder K√§sest√ºcke\n",
    "\n",
    "re.findall(r\"\\b(M√§use|K√§sest√ºcke)\\b\", chat_gpt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auswahl und Gruppierung kombiniert\n",
    "marks = re.findall(r\"[\\\"\\'](.........)[\\\"\\']\", chat_gpt_text)      # Backslash hebt Bedeutung als Sonderzeichen auf\n",
    "for mark in marks:\n",
    "    print(mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suche nach W√∂rtern, die mit \"b\" beginnen, gefolgt von beliebigem Wortzeichen\n",
    "\n",
    "re.findall(r\"\\bb\\w\", chat_gpt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finden von einzelnen Zahlen\n",
    "\n",
    "re.findall(r\"\\b\\d\\b\", chat_gpt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finden von Zeichenfolgen, die aus genau zwei Zeichen bestehen\n",
    "re.findall(r\"\\b\\w\\w\\b\", chat_gpt_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finden der Zeichenfolge \"Katze123\"\n",
    "\n",
    "re.findall(r\"\\b\\w\\w\\w\\w\\w\\d\\d\\d\\b\", chat_gpt_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K√∂nnen Sie das Muster des nachfolgenden Regul√§ren Ausdrucks beschreiben und prognostizieren, welche Art von R√ºckgabe kommen wird?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle erst ausf√ºhren, wenn obige Frage beantwortet wurde\n",
    "regex_structure = re.findall(r\"\\b\\w\\w[s|n]\\b\", chat_gpt_text) \n",
    "print(regex_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K√∂nnen Sie das Muster des nachfolgenden Regul√§ren Ausdrucks beschreiben und prognostizieren, welche Art von R√ºckgabe kommen wird?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle erst ausf√ºhren, wenn obige Frage beantwortet wurde\n",
    "some_short_words = re.findall(r\"\\b[A-Z].[s|n]\\b\", text) \n",
    "print(some_short_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multipliers\n",
    "Was nun noch fehlt, ist die M√∂glichkeit darzustellen, dass bestimmte Teilausdr√ºcke wiederholt werden sollen. Um die Jahreszahlen aus dem Textauszug des Tagungsberichts auszulesen, sah Ihr Regul√§rer Ausdruck oben wahrscheinlich ungef√§hr so aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years_option_a = re.findall(r\"\\d\\d\\d\\d\", text) \n",
    "print(years_option_a)\n",
    "\n",
    "years_option_b = re.findall(r\"[0-9][0-9][0-9][0-9]\", text)\n",
    "print(years_option_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mit sogenannten Multiplikatoren oder Quantoren k√∂nnen Sie diese Ausdr√ºcke vereinfachen:\n",
    "- `{m}` = gibt an, dass genau *m* Zeichenfolgen des voranstehenden Regul√§ren Ausdrucks gematcht werden sollen\n",
    "\n",
    "Angewendet auf die Anforderung, alle Jahreszahlen aus dem Textauszug auszulesen, sieht der verk√ºrzte Regul√§re Ausdruck nun wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = re.findall(r\"\\d{4}\", text) \n",
    "print(years)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es ist auch m√∂glich eine Range aus Minimal- und Maximalwiederholung anzugeben:\n",
    "- `{m,n}` = gibt an, dass mindestens *m* und maximal *n* Zeichenfolgen des voranstehenden Regul√§ren Ausdrucks gematcht werden sollen\n",
    "\n",
    "Wenn wir aus dem Textauszug Zahlen mit mindestens zwei und maximal drei Zeichenfolgen auslesen wollen, sieht der entsprechende Code wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = re.findall(r\"\\d{2,3}\", text) \n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Es kann auch m√∂glich sein, dass wir nach einem Muster suchen, das bestimmte Substrings enthalten kann, aber nicht muss. Hierf√ºr steht das `?` zur Verf√ºgung. In der Notation der Range entspricht es dem Zusatz `{0,1}`. Der Ausdruck r\"\\beine?\\b\" liefert beispielsweise √úbereinstimmungen sowohl f√ºr \"ein\" als auch \"eine\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = re.findall(r\"\\beine?\\b\", text)\n",
    "print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `*` bedeutet, dass das voranstehende Zeichen beziehungsweise der voranstehende Teilausdruck keinmal oder beliebig oft vorkommen kann. In der Notation der Range entspricht das dem Zusatz `{0,}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = re.findall(r\"\\d\\d*\", text)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `+` bedeutet, dass das voranstehende Zeichen beziehungsweise der voranstehende Teilausdruck mindestens einmal oder beliebig oft vorkommen kann. In der Notation der Range entspricht das dem Zusatz `{1,}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = re.findall(r\"\\d+\", text)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beschreiben Sie die Muster des nachfolgenden Regul√§ren Ausdrucks. Welches Ergebnis erwarten Sie?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ending = re.findall(r\"\\w+g\\b\", text)\n",
    "print(word_ending)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beschreiben Sie die Muster des nachfolgenden Regul√§ren Ausdrucks. Welches Ergebnis erwarten Sie?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_endings = re.findall(r\"\\w+[g|n]\\b\", text)\n",
    "print(word_endings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beschreiben Sie die Muster des nachfolgenden Regul√§ren Ausdrucks. Welches Ergebnis erwarten Sie?** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specific_words = re.findall(r\"Vor\\w+\", text)\n",
    "print(specific_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alle S√§tze auslesen und in Liste speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract all sentences\n",
    "sentences = re.findall(r\".+?\\.\", text)   # Backslash escaped die Sonderfunktion der Metazeichen. Der Punkt ist hier nicht mehr Platzhalter f√ºr beliebiges Zeichen, sondern Zeichen f√ºr das Satzende.\n",
    "for each in sentences:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìù **Jetzt:** √úbung zur Datenbereinigung mit Regul√§ren Ausdr√ºcken\n",
    "\n",
    "Nachdem Sie nun die Basics im Umgang mit Regul√§ren Ausdr√ºcken kennengelernt haben, sollen Sie auf Basis des gesamten [Tagungsberichts](https://www.hsozkult.de/conferencereport/id/fdkn-125120) nach einigen Mustern suchen und daf√ºr Regul√§re Ausdr√ºcke formulieren.\n",
    "\n",
    "Extrahieren Sie\n",
    "- alle Zahlen\n",
    "- alle Anmerkungen, wie zum Beispiel \"[1]\"\n",
    "- alle W√∂rter, die mit einem \"A\" oder \"a\" beginnen \n",
    "- alle Bestandteile, die in Anf√ºhrungszeichen gesetzt sind (sowohl einfache als auch doppelte - achten Sie hierbei auf die spezifische Form der Anf√ºhrungszeichen)\n",
    "- Pr√ºfen Sie au√üerdem, ob es hier doppelte Leerzeichen gibt und wenn ja wie oft\n",
    "- Entfernen Sie alle Anmerkungen und √ºberfl√ºssigen Leerzeichen. Weisen Sie den Text einer neuen Variablen zu.\n",
    "\n",
    "‚è≥ 30 Minuten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"\"\"Der Arbeitskreis ‚ÄûDigital Humanities Munich‚Äú (dhmuc)[1] widmete seinen ersten Workshop 2016 dem Thema ‚ÄûDigitale Daten in den Geisteswissenschaften. Interdisziplin√§re Perspektiven f√ºr semantische und strukturelle Analysen‚Äú.[2] Insgesamt 14 Vortr√§ge er√∂rterten aktuelle Forschungen und Infrastrukturen im Bereich der maschinellen Textanalyse.\n",
    "Folgende Institutionen zeichneten f√ºr die Organisation des Workshops verantwortlich: dhmuc ‚Äì Arbeitskreis ‚ÄúDigital Humanities Munich‚Äù, das Institut f√ºr Computerlinguistik Universit√§t Z√ºrich [3], das Historische Seminar der Ludwig-Maximilians-Universit√§t M√ºnchen [4], die IT-Gruppe Geisteswissenschaften (ITG) der Ludwig-Maximilians-Universit√§t M√ºnchen [5] sowie die Bayerische Akademie der Wissenschaften [6].\n",
    "\n",
    "Einf√ºhrung\n",
    "ECKHART ARNOLD (M√ºnchen), MARK HENGERER (M√ºnchen), NOAH BUBENHOFER (Z√ºrich) und CHRISTIAN RIEPL (M√ºnchen) machten klar, dass das Vorhandensein grosser Mengen an digitalen Textkorpora den Geisteswissenschaften neue Zukunftsperspektiven er√∂ffnet, gleichzeitig aber auch neue Herausforderungen an die Disziplinen stellt. Der Br√ºckenschlag zwischen Philologie und Informationstechnologie erfordert von den Forschenden ein hohes Mass an Technikaffinit√§t. Das Verst√§ndnis von Text als Daten und dessen interaktive graphische Visualisierung ver√§ndert die hermeneutischen Herangehensweisen und Methoden.\n",
    "\n",
    "Panel 1: Korpora\n",
    "Im Er√∂ffnungsreferat stellte PETER MAKAROV (Z√ºrich) sein PhD Projekt zum Thema ‚ÄûTowards automated content event analysis: Mining for protest events‚Äú vor. Es ist Teil des POLCON Projekts unter Professor Hanspeter Kriesi.[7] Das Projekt bewegt sich zwischen Politikwissenschaft und Computerlinguistik. Ziel ist, herauszufinden, inwiefern moderne Natural Language Processing Technik die thematische Extraktion von Daten aus dem Internet unterst√ºtzen kann. Konkret geht es in der Studie um die Extraktion von News-Texten im Zusammenhang mit √ñffentlichen Protesten.\n",
    "Methodisch setzt das Projekt auf Machine-Learning-Verfahren. Hierzu m√ºssen geeignete Klassifizierungsmodelle erstellt und Entwicklungszyklen implementiert werden, mit deren Hilfe Trainings-Korpora erstellt werden k√∂nnen. In einem iterativen Prozess werden diese Trainingsdaten annotiert und verbessert. Dabei hat sich gezeigt, dass linguistische Standardmodelle zur Annotation nicht geeignet waren, um die Identifikation von Textstellen, die Protest indizierten, zu verbessern. Daher werden vereinfachte, besser auf die konkrete Fragestellung zugeschnittene Annotationssysteme entwickelt.\n",
    "DANIEL KNUCHEL (Z√ºrich) referierte unter dem Titel ‚ÄûHIV/AIDS diskurslinguistisch ‚Äì ein multimediales Korpus‚Äú √ºber sein Promotionsprojekt. In diesem analysiert er, welches Wissen heute zum Thema HIV/AIDS in der √ñffentlichkeit zirkuliert. Dazu baute der Referent ein Korpus aus unterschiedlichen Quellen (Massenmedien, Selbsthilfe-Blogs, Social Media) auf. Er wies darauf hin, dass bei solchen Vorhaben nebst den konzeptuellen und technischen Fragen die rechtlichen Bedingungen zur Datennutzung fr√ºhzeitig gekl√§rt werden m√ºssen. Wichtig sei zudem, dass von Anfang an auf Nachhaltigkeit geachtet werde, um sp√§tere Nutzungen der Daten zu erm√∂glichen.\n",
    "MAX HADERSBECK (M√ºnchen) berichtete in seinem Vortrag √ºber die Erfahrungen mit der FinderApp ‚ÄúWittFind‚Äú.[8] Die webbasierte Applikation steht Forschenden nun seit vier Jahren zur Verf√ºgung, um den Open Access zug√§nglichen Teil des Nachlasses von Ludwig Wittgenstein nach W√∂rtern, Phrasen, S√§tzen und semantischen Begriffen zu durchsuchen. Sie setzt dazu regelbasierte computerlinguistische Verfahren ein. WittFind zeichnet sich dadurch aus, dass die gefundenen Textstellen zusammen mit dem Faksimileextrakt dargestellt werden und eine √úberpr√ºfung anhand des Originaltexts jederzeit m√∂glich ist.\n",
    "Unter anderem am Beispiel des englischen Neologismus ‚Äúcherpumple‚Äù [9] stellte SUSANNE GRANDMONTAGNE (M√ºnchen), das System ‚ÄûNeoCrawler‚Äú [10] vor. NeoCrawler verfolgt die Entstehung und Verbreitung von Neologismen im Internet. Die Ergebnisse werden automatisiert f√ºr weitere linguistische Analysen aufbereitet. Zudem stellt das System Zeitreihenverlaufsanalysen zur Verf√ºgung. Eine Benutzeroberfl√§che unterst√ºtzt die manuelle Kategorisierung der erhobenen Daten. Aus rechtlichen Gr√ºnden kann diese M√∂glichkeit jedoch nicht im Rahmen von Crowd Sourcing genutzt werden.\n",
    "Die prosopographische Erforschung der Herrschaftselite der Habsburgermonarchie steht im Zentrum des Projekts ‚ÄûKaiserhof‚Äú [11], das MARK HENGERER und GERHARD SCH√ñN (M√ºnchen) vorstellten. Nebst der eindeutigen Identifikation von Personen ist die Messbarkeit qualitativer Aspekte eine der haupts√§chlichen Herausforderungen, um Netzwerke und ‚ÄûReichweiten von Integration‚Äú visualisieren zu k√∂nnen. Dabei hat sich der Ansatz bew√§hrt, von Begriffen mittlerer Reichweite auszugehen. Visualisierungen (etwa von Verwandtschaftsbeziehungen oder Geolokalisierungen) sind in diesem Zusammenhang von hohem heuristischem Wert.\n",
    "\n",
    "Natural Language Processing / Suche\n",
    "Zeitangaben sind eine zentrale Information in historischen Dokumenten. Das war der Ausgangspunkt f√ºr die Pr√§sentation von NATALIA KORCHAGINA (Z√ºrich) zu ‚ÄúNatural language processing for historical documents‚Äú. Doch die maschinelle Textextraktion aus den oft handschriftlichen Dokumenten ist komplex. Ziel des Forschungsvorhabens der Referentin ist es, ein Tool f√ºr die automatisierte Extraktion von Zeitangaben aus historischen Texten zu entwickeln. Als Quellengrundlage dient ein Korpus von digitalisierten Schweizer Rechtstexten zwischen dem 10. und 18. Jahrhundert.\n",
    "In einer ersten Phase des Projekts wurde unter Nutzung des an der Universit√§t Heidelberg entwickelten Zeittaggers HeidelTime [12] ein fehlerfreies, aber kleines manuell annotiertes Gold Standard Korpus erstellt. Auf dieser Grundlage wird sodann mit einem hybriden Vorgehen, das machine-learning und regelbasierte Methoden kombiniert, ein gr√∂sseres Silver Standard Korpus erarbeitet, das f√ºr die Extraktion von Zeitangaben herangezogen werden kann.\n",
    "Zeitgen√∂ssische Rechtstexte standen im Zentrum des Forschungsprojekts von KYOKO SUGISAKI (Z√ºrich). Sie pr√§sentierte in ihrem  Vortrag unter dem Titel ‚ÄúNatural language processing in speziellen Textsorten, z.B. legislative Texte‚Äú ihre soeben abgeschlossene Doktorarbeit. Am Beispiel von online verf√ºgbaren Schweizer Gesetzestexten erstellte sie ein qualitativ hochwertiges Korpus von fachspezifischen Texten. Im Verlauf der Arbeiten zeigte sich, dass vorhandene Referenzkorpora (etwa die Sammlung Schweizerischer Rechtsquellen) genutzt werden k√∂nnen, jedoch an die Spezifika des Vorhabens angepasst werden m√ºssen. Mittels Kombination von verschiedenen Analyseverfahren und Tools (u.a. POS-tagging, morphosyntaktische Analyse, Style-Checking) konnte die Qualit√§t der Texterkennung deutlich verbessert werden.\n",
    "\n",
    "Visualisierung\n",
    "Unter dem Titel ‚ÄûVisualisierungen in den Digital Humanities‚Äú diskutierten NOAH BUBENHOFER, KLAUS ROTHENH√ÑUSLER und DANICA PAJOVIC (alle Z√ºrich) die theoretischen Grundlagen von Visualisierungen und hinterfragten g√§ngige Visualisierungspraktiken in den Digital Humanities. Ausgangspunkt ist die Feststellung, dass Visualisierungen nicht nur bei der Darstellung von Analyseergebnissen, sondern auch bei der Datenexploration eine wichtige Rolle spielen.\n",
    "Besonders bei explorativen Visualisierungen sind gem√§ss den  Referenten methodisch-technische Aspekte wichtig. Denn Diagramme sind nicht Bilder. Sie sind hoch abstrahierte Darstellungen, die Hypothesen √ºber Sachverhalte transportieren. Visualisierungen lassen sich entlang einer Reihe von grafischen, datentypischen, diagrammatischen, semiotischen, √§sthetischen, technischen und diskursiven Attributen kategorisieren und beurteilen.\n",
    "Anhand der Darstellung von Geokollokationen, das hei√üt von sprachlichen √Ñu√üerungen √ºber Orte zeigte Noah Bubenhofer, wie durch Sprechen eine Vorstellung von Welt konstruiert wird.[13] Die naheliegende Visualisierung von Geokollokationen auf einer Weltkarte ist dann ein voraussetzungsvoller Vorgang, der unhinterfragte Pr√§missen von Kartendarstellungen √ºbernimmt. Bubenhofer pl√§dierte daher daf√ºr, auch nicht kartenbasierte Visualisierungen in Betracht zu ziehen. Das Beispiel illustrierte, wie Denkstile,    Software und technische M√∂glichkeiten in Visualisierungen mit einfliessen und diese in gewisser Weise vorbestimmen.\n",
    "MATTHIAS REINERT (M√ºnchen) pr√§sentierte in seinem Referat ‚Äûdeutsche-biographie.de ‚Äì ein historisch biografisches Informationssystem. Computerlinguistischer Ansatz und Visualisierung‚Äú, das aus diesem Vorhaben resultierende Internetangebot.[14] In rund 48.000 Lexikonartikeln bietet es Informationen zu rund 540.000 Personen. F√ºr die zuverl√§ssige Volltexterfassung und -kodierung sowie den Normdatenabgleich von Personen und Orten wurden seit 2012 computerlinguistische Verfahren eingesetzt. Hierzu wurden lokale Grammatiken und eine korpora-spezifische Wortdatenbank erstellt. Das historisch-biografische Informationssystem erm√∂glicht eine Geo-Visualisierung und die Darstellung von Ego-Netzwerken. In der Diskussion betonte der Referent, dass im Zusammenhang mit einem solchen interaktiven Angebot eine transparente Kommunikation √ºber die M√∂glichkeiten und Grenzen des Systems und der Datenbasis unerl√§sslich ist, um den Nutzern die Einsch√§tzung der Evidenz der gewonnen Resultate zu erm√∂glichen.\n",
    "Unter dem Titel ‚ÄûTheatrescapes‚Äú [15] argumentierte TOBIAS ENGLMEIER (M√ºnchen), dass die stetig wachsenden, nun auch f√ºr die Geisteswissenschaften verf√ºgbaren Datenbest√§nde oft nur mit Techniken der Informationsvisualisierung erfasst werden k√∂nnten und interpretierbar seien. Das Projekt ‚ÄûTheatrescapes. Mapping Global Theatre Histories‚Äú nutzt f√ºr die interaktive Kartendarstellungen WebGL (Web Graphics Library) [16] und den Google Maps API [17]. Der Referent zeigte auf, dass mittels dieses pragmatischen Ansatzes die technischen H√ºrden bei der Georeferenzierung von grossen Datenbest√§nden mit vertretbarem Aufwand √ºberwunden und ansprechende Resultate wie etwa interaktive historisierte Kartendarstellungen erzielt werden k√∂nnen. Allerdings betonte er, wie schon Noah Bubenhofer vor ihm, dass Entscheidungen √ºber den Einsatz von bestimmter Software √ºber technische Aspekte hinausreichen und auch inhaltliche Konsequenzen haben.\n",
    "EMMA MAGES (M√ºnchen) stellte den ‚ÄûAudioatlas Siebenb√ºrgisch-S√§chsischer Dialekte‚Äú (ASD) vor, einen interaktiven Online-Atlas.[18] Er erschlie√üt eine umfangreiche Audiodokumentation deutscher Ortsdialekte Siebenb√ºrgens und der Marmarosch und macht diese in Transkription und Audioaufnahmen zug√§nglich. Nebst der eigentlichen Transkription wurde eine morphosyntaktische Etikettierung vorgenommen und eine Ontologie f√ºr die Erschliessung entworfen. Mittels Kartierung erlaubt der ASD unter anderem qualitative und quantitative Sichten auf die √∂rtliche Verteilung der Dialekte.\n",
    "In seinem Referat √ºber die Online-Plattform ‚ÄûVerbaAlpina‚Äú berichtete STEPHAN L√úCKE (M√ºnchen) √ºber die Herausforderungen, die sich bei diesem politische Grenzen √ºberschreitenden und mehrsprachigen Ansatz stellten.[19] Ziel des Langzeitprojekts ‚ÄûVerba Alpina‚Äú ist es, den sprachlich stark fragmentierten Alpenraum in seiner kultur- und sprachgeschichtlichen Zusammengeh√∂rigkeit zu erschliessen. Das Projekt fokussiert auf die Wechselbeziehung (sowohl in onomasiologischer wie semasiologischer Perspektive) zwischen W√∂rtern und bezeichneten Konzepten. Die sprachlichen Zusammenh√§nge werden erg√§nzt mit ethnographischen, historischen und politischen Aspekten und in einer interaktiven Online-Karte mit Crowd-Sourcing-Komponente dargestellt.\n",
    "\n",
    "Crowd\n",
    "Das Referat ‚ÄûText+Berg ‚Äì 150 Jahre alpinistische Texte: OCR-Fehler, Crowd Correction‚Äú von SIMON CLEMATIDE (Z√ºrich) diskutierte die Voraussetzungen f√ºr erfolgreiches Crowd Sourcing.[20] Im Rahmen des Projekts Text+Berg realisierte das Institut f√ºr Computerlinguistik der Universit√§t Z√ºrich eine Online-Plattform zur Korrektur des OCR-Textes der digitalisierten Jahrb√ºcher des Schweizerischen Alpenklubs (SAC) von 1864 bis 1899.[21] Entscheidend f√ºr die Motivierung von Freiwilligen und damit f√ºr den Erfolg des Vorhabens waren eine sorgf√§ltig programmierte Benutzeroberfl√§che und ein einfacher Workflow. Dazu kamen begleitende Massnahmen, um die Motivation der Teilnehmenden aufrechtzuerhalten. Hierzu geh√∂rten spielerische Elemente und Layout-Massnahmen, die den Teilnehmenden R√ºckmeldungen zum Datenzustand und zur geleisteten Arbeit geben. Ein Vorteil war, dass es sich beim potentiellen Teilnehmerkreis um gut organisierte und am Thema interessierte Vereinsmitglieder mit hoher Sachkenntnis handelte.\n",
    "Abschliessend stellte GERHARD SCH√ñN (M√ºnchen) das Projek ‚ÄûPlay4Science‚Äú [22] und die in diesem Rahmen entwickelte Spiel-Plattform ‚ÄûArtigo‚Äú vor. Das Projekt bringt Geisteswissenschaftler/innen, Informatiker/innen und Computerlinguist/innen zusammen, um zweckgerichtete soziale Spiel-Software (‚ÄûGames with a Purpose‚Äú (GWAP)) zu entwickeln, die seit einiger Zeit auch im wissenschaftlichen Bereich erfolgreich Crowd-Sourcing-Ans√§tze unterst√ºtzen. Ziel von Play4Science ist, eine anpassbare universelle Plattform anzubieten, die von allen F√§chern f√ºr verschiedenste Anwendungen sozialer Software genutzt werden kann.\n",
    "Die bereits realisierte Spiel-Plattform ‚ÄûArtigo‚Äú [23] l√§dt zur Verschlagwortung von Gem√§lden aus einer Bilddatenbank ein. Sie schaltet zwei Mitspieler zusammen, die unabh√§ngig voneinander relevant erscheinende Begriffe f√ºr dieselben Bilder eingeben. Die solcherart Peer-validierten Begriffe werden in der Datenbank gespeichert und sind f√ºr sp√§tere Suchabfragen nutzbar.\n",
    "\n",
    "Fazit\n",
    "Der Workshop bot einen guten √úberblick  √ºber den State-of-the-Art computerlinguistischer Ans√§tze f√ºr die digitale Aufbereitung von Textkorpora. Er wies auf die Herausforderungen hin, die sich bei der interdisziplin√§ren Zusammenarbeit an der Schnittstelle zwischen Technik und geisteswissenschaflticher Forschung stellen. Es wurde klar, dass eine fundierte Fragestellung entscheidend f√ºr den Erfolg von computerlinguistischen Vorhaben ist. Zugleich wurde aber unter dem Stichwort ‚Äûcode matters‚Äú auch betont, dass technologische Aspekte nicht vernachl√§ssigt werden d√ºrfen, da sie Einfluss auf Vorgehensweisen und Resultate haben. Von den Geisteswissenschaftlern muss daher verlangt werden, dass sie wissen, was die verwendeten Algorithmen tun. Dies gilt insbesondere auch f√ºr heuristisch und explorativ eingesetzte Visualisierungen, bei denen sich die Forschenden stets zu fragen haben, ob sie den generierten Visualisierungen genug kritisch gegen√ºberstehen. Unter geisteswissenschaftlichen Vorzeichen kann eine in den Visual Analytics mitunter unterstellte korrelationsbasierte ‚Äûground truth‚Äú nicht vorausgesetzt werden.\n",
    "In den Diskussionen hat sich ferner die Sicherstellung der Nachhaltigkeit in computerlinguistischen Vorhaben als zentraler Aspekt herausgestellt. Dabei geht es um mehr als Datenverf√ºgbarkeit. Entscheidend sind das Bewusstsein f√ºr die Br√ºchigkeit der Datengrundlagen und der Umgang mit Unsch√§rfen und Unvollkommenheiten. In einer weiteren Perspektive identifizierte der Workshop eine Reihe zentraler Erfolgsfaktoren f√ºr Digital Humanities-Projekte. So gilt es die rechtlichen Bedingungen f√ºr die Datennutzung fr√ºhzeitig zu kl√§ren, eine ausbauf√§hige Infrastruktur aufzubauen, die Mitarbeitenden auszuw√§hlen, auszubilden und zu begeistern sowie die langfristige Finanzierung sicherzustellen. Insgesamt bot die Tagung einen guten √úberblick √ºber die M√∂glichkeiten der maschinellen Analyse und Interpretation von Texten. Es herrschte Konsens dar√ºber, dass sich die Geisteswissenschaften dadurch in den kommenden Jahren stark ver√§ndern werden.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ihre L√∂sung"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
