{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0ce7041-55e4-4c10-b894-2979ae08c7da",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# üíª spaCy Basics: Eine Einf√ºhrung in die Grundlagen des NLP-Frameworks\n",
    "\n",
    "√úber das Notebook hinausgehende grundlegende Ressourcen zu [spaCy](https://spacy.io/) sind √ºber folgende Links zu erreichen:\n",
    "\n",
    "- Dokumentation und Tutorials: https://spacy.io/usage\n",
    "- √úbersicht zu den Sprachmodellen: https://spacy.io/models bzw. https://spacy.io/models/de\n",
    "- API: https://spacy.io/api\n",
    "- Kurs - Modernes NLP mit spaCy: https://course.spacy.io/de\n",
    "- Video Tutorials: https://www.youtube.com/c/ExplosionAI\n",
    "- Sammlung von weiteren Ressourcen zu spaCy: https://spacy.io/universe, https://explosion.ai/software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d17088-4ccc-4c09-bb64-b63400e6c39c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Download der deutschen Sprachmodelle\n",
    "\n",
    "SpaCy stellt zahlreiche Sprachmodelle in verschiedenen Sprachen und Gr√∂√üen zur Verf√ºgung. Hier eine √úbersicht zu den aktuell verf√ºgbaren deutschen Sprachmodellen:\n",
    "\n",
    "| Typ | Name | Gr√∂√üe | Link |\n",
    "|-------|------| ------|------|\n",
    "| small | de_core_news_sm | 13 MB | https://spacy.io/models/de#de_core_news_sm |\n",
    "| medium | de_core_news_md | 42 MB | https://spacy.io/models/de#de_core_news_md |\n",
    "| large | de_core_news_lg | 541 MB | https://spacy.io/models/de#de_core_news_lg |\n",
    "| transformer-based | de_dep_news_trf | 391 MB | https://spacy.io/models/de#de_dep_news_trf |\n",
    "\n",
    "Der Download im Notebook erfolgt durch den nachfolgenden Befehl; hier wird das deutsche Small-Modell heruntergeladen. In angewandten Kontexten im Nachgang des Workshops ist die Arbeit mit den gr√∂√üeren Sprachmodellen zu empfehlen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc48ddd5-5b57-4e15-b594-066fa49c3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bitte diese Codezelle ausf√ºhren und das Medium-Sprachmodell herunterladen\n",
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b55d1f-371d-42a5-896f-bed5250debc3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Import\n",
    "\n",
    "Nach dem Import der Bibliothek wird die Version gecheckt: Die spaCy-Version der Bibliothek muss mit der Versionsnummer des Sprachmodells √ºbereinstimmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f03bc33-1447-4e77-a1e8-e753abdc845b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03912ad-03a1-40d9-adca-fdcfe3f4d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check version\n",
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c21d64-8c1c-4820-a75d-42f2f43ab003",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Informationen zum Sprachmodell\n",
    "\n",
    "### Laden des Sprachmodells\n",
    "\n",
    "Nach Konvention wird die Variable `nlp` genannt, die das geladene Sprachmodell enth√§lt. Einzelne Komponenten k√∂nnen √ºber den Parameter `disable` vom Laden ausgeschlossen werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288afd5e-2aac-4024-9e14-92c08519d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('de_core_news_sm') \n",
    "\n",
    "# Zum Ausschalten einzelner Komponenten\n",
    "# nlp = spacy.load('de_core_news_md', disable=['parser', 'tagger'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8d9510",
   "metadata": {},
   "source": [
    "### Metadaten zum Sprachmodell\n",
    "\n",
    "Mit den folgenden Befehlen lassen sich Informationen zu den Metadaten des Sprachmodells ausgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e867a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pfad, in dem das Sprachmodell liegt\n",
    "nlp.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metadaten zum Sprachmodell\n",
    "nlp.meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c93123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informationen zum Sprachmodell\n",
    "print(nlp.meta[\"name\"])\n",
    "print(nlp.meta[\"lang\"])\n",
    "print(nlp.meta[\"version\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a25c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beschreibung der Pipeline\n",
    "nlp.meta[\"description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8922d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzeigen der verf√ºgbaren NER-Labels\n",
    "nlp.meta[\"labels\"][\"ner\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b906e412",
   "metadata": {},
   "source": [
    "Mit `spacy.explain()` lassen sich einzelne Bezeichnungen, die von spaCy genutzt werden, n√§her erl√§utern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd71e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy explain\n",
    "spacy.explain(\"ORG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36125b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in nlp.meta[\"labels\"][\"ner\"]:\n",
    "    print(f\"{ent}: {spacy.explain(ent)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377d818f-b0d1-4c6b-90db-58cc611523c8",
   "metadata": {},
   "source": [
    "## spaCy im Einsatz: Einstieg in Named Entity Recognition als ein Beispiel\n",
    "\n",
    "### Textdaten einlesen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71dae96-0027-4923-b745-0ec668a2c916",
   "metadata": {},
   "source": [
    "Der Text aus der Datei `kafka_verwandlung_1915.txt` ist entnommen von: \n",
    "\n",
    "> Kafka, Franz: Die Verwandlung. Leipzig, 1915. In: Deutsches Textarchiv <https://www.deutschestextarchiv.de/kafka_verwandlung_1915/1>, abgerufen am 19.02.2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../daten/kafka_verwandlung_1915.txt\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a4dbd3-2245-418a-af71-d3a10abccf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18a78bb-c4a9-4e3b-a606-603af6543c1e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Doc-Objekt erstellen\n",
    "\n",
    "Wenn wir das Sprachmodell auf den Text anwenden, dann erhalten wir ein `Doc-Objekt`. Dieses stellt uns nun  alle Informationen bereit, die √ºber spaCys Pipeline-Komponenten dem Text hinzugef√ºgt wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc52de7-d8f0-436c-9577-694b902dfc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is were the magic happens!\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50f756e-7692-4084-84a1-670809b7d380",
   "metadata": {},
   "source": [
    "### Informationen im Token-Objekt\n",
    "\n",
    "Neben dem `Doc-Objekt` sind auch die `Token-Objekte` ein wichtiger Baustein in der Architektur von spaCy. Alle verf√ºgbaren Attribute, die sich beim `Token-Objekt` abrufen lassen finden sich hier: https://spacy.io/api/token#attributes . Nachfolgend eine kleine √úbersicht zu den Attributen mit einer Erl√§uterung mittels `spacy.explain()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b466fb-24eb-49be-8655-ad628a819e73",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for token in doc[:20]:\n",
    "    print(f\"\"\"Token: {token}  \n",
    "              Lemmata: {token.lemma_} \n",
    "              POS: {token.pos_}\n",
    "              explain POS: {spacy.explain(token.pos_)}\n",
    "              TAG: {token.tag_}\n",
    "              explain TAG: {spacy.explain(token.tag_)}\n",
    "              Dependency: {token.dep_}\n",
    "              explain Dependency: {spacy.explain(token.dep_)}\n",
    "              Entity: {token.ent_type_}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef8306b-e898-444a-93e7-ad3a9848edd1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Informationen im Doc-Objekt\n",
    "\n",
    "Auch √ºber das `Doc-Objekt` k√∂nnen wir auf verschiedene hilfreiche Informationen zugreifen. N√§heres hierzu unter: https://spacy.io/api/doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39380514-28b4-4a30-969f-2e85e90b7682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datentyp\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bbb8a8-0dbc-44e2-bfc2-f99bc8948ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprache des verwendeten Sprachmodells\n",
    "doc.lang_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da0aff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl der Token im Doc-Objekt\n",
    "print(doc.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2563d",
   "metadata": {},
   "source": [
    "#### S√§tze extrahieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a607d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Ausgabe der S√§tze\n",
    "for sent in doc.sents:\n",
    "    print(sent)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7980d5-f44c-4cdf-9d74-037e3d6ecdae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### [Named Entities](https://spacy.io/api/entityrecognizer) identifizieren (out-of-the-box)\n",
    "\n",
    "Es gibt verschiedene M√∂glichkeiten, auf die im Text erkannten Entit√§ten zuzugreifen.\n",
    "\n",
    "##### `doc.ents`\n",
    "\n",
    "Die Entit√§ten sind hier als Datentyp `span` in einem Tupel angelegt. Die Span kann √ºber den entsprechenden Text und das Label repr√§sentiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840398b6-2874-459d-b8eb-c6b751029040",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(doc.ents))           # Datentyp des .ents-Attributs\n",
    "print(len(doc.ents))            # Anzahl der erkannten NEs\n",
    "print(doc.ents)                 # Ausgabe des Tupels der erkannten Entit√§ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78becec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der erkannten Entit√§ten mit den entsprechenden Indexpositionen (auf Token-Ebene)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start, ent.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42964ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3443fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabe der erkannten Entit√§ten mit dem zugeh√∂rigen Entity-Label\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7730ede4",
   "metadata": {},
   "source": [
    "√úber das Attribut `.label_` k√∂nnen z.B. einzelne Kategorien von Named Entities mit Hilfe einer for-Schleife angezeigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122716f1-fc33-44cd-ba2e-dc8561e3d054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nur Entit√§ten anzeigen, die als ORG gelabelt wurden\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'ORG':\n",
    "        print(ent.text, ent.label_, )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaece8ec",
   "metadata": {},
   "source": [
    "##### Zugriff auf Entit√§ten √ºber `Token-Objekt` \n",
    "\n",
    "Auf der Token-Ebene lassen sich √ºber weitere Attribute etwa die Info zur Position im IOB-Format mit `.ent_iob_` abrufen.\n",
    "\n",
    "IOB ist ein etabliertes Annotationsschema f√ºr die Auszeichnung von NEs. Es steht f√ºr *Inside*, *Outside*, *Beginning*:\n",
    "- **I (Inside):** Das Token ist Teil einer Entit√§t, aber nicht das erste Token dieser Entit√§t.\n",
    "- **O (Outside):** Das Token ist nicht Teil einer benannten Entit√§t.\n",
    "- **B (Beginning):** Das Token ist der Anfang einer Entit√§t.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49cb59a-c9c6-4038-8aec-c673f98b6a3f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pr√ºfen der IOB-Label f√ºr PER-Entit√§ten\n",
    "\n",
    "for token in doc:\n",
    "    if token.ent_type_ == \"PER\":\n",
    "        print(f\"Token: {token}, \\n\\tEntity: {token.ent_type_}, IOB-Positon: {token.ent_iob_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fced8619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zum Vergleich f√ºr alle im Text erkannten Token \n",
    "\n",
    "for token in doc:\n",
    "    print(f\"Token: {token}, \\n\\tEntity: {token.ent_type_}, IOB-Positon: {token.ent_iob_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c726c1d4",
   "metadata": {},
   "source": [
    "Um eine √úbersicht der in dem Text einmal vorkommenden Named Entities einer Kategorie zu erhalten, kann man mit Sets arbeiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c289be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl der unique PER-Entities ermitteln\n",
    "\n",
    "persons = set()\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\":\n",
    "        persons.add(ent.text)\n",
    "\n",
    "print(len(persons))\n",
    "print(persons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anzahl der unique Entit√§ten-Typen ermitteln\n",
    "\n",
    "for entity in [\"PER\", \"ORG\", \"LOC\", \"MISC\"]:\n",
    "    print(entity)\n",
    "    print(len({ ent.text for ent in doc.ents if ent.label_ == entity }))               # hier wird eine Set-Comprehension verwendet, damit k√∂nnen wir den Code aus der vorherigen Codezelle in einer Zeile schreiben; f√ºr den Anfang ist es empfehlenswert, alles auszuschreiben\n",
    "    print(sorted(list({ ent.text for ent in doc.ents if ent.label_ == entity })))      # Das per Set-Comprehension generierte Set wird in eine Liste umgewandelt und sortiert\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0b6dcf-5959-451e-87ec-22199972275d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Visualisieren der Entities\n",
    "\n",
    "SpaCy erm√∂glicht es, mit einem einfachen Befehl, die Named Entities in dem Text zu visualisieren. Erfolgt die Darstellung in einem JupyterNotebook, dann muss der Parameter `jupyter=True` gesetzt werden. Die Visualisierung des Textes l√§sst sich zudem als html-Datei speichern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de0596d-eb6a-4786-bada-69522913771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54f0561-9327-49ce-9d21-416a5e7dc094",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89015ede-53ac-49b3-abe2-7afe4eaa3c2f",
   "metadata": {},
   "source": [
    "### Speichern der Visualisierung in html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666a7f7-13f9-4988-a345-c3fcc50f88fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "html = displacy.render(doc, style=\"ent\", page=True, jupyter=False)\n",
    "with open(\"../daten/output/visu-example-kafka_verwandlung_1915.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b89eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73309a38",
   "metadata": {},
   "source": [
    "### üìù **Jetzt:** Aufgabe 1 - Sprachmodelle vergleichen\n",
    "\n",
    "- Laden Sie das Sprachmodell `de_core_news_md` herunter und erzeugen Sie f√ºr \"Die Verwandlung\" mit dem Medium-Sprachmodell ein neues Doc-Objekt.\n",
    "- Visualisieren Sie erneut die Entit√§ten f√ºr den Text und speichern Sie die Visualisierung als HTML-Datei.\n",
    "- Vergleichen Sie die NER-Ergebnisse des kleinen und mittleren Sprachmodells.\n",
    "\n",
    "*Hinweis: In den nachfolgenden beiden Aufgaben werden dieselben Arbeitsschritte noch mal auf andere Texte angewendet.* \n",
    "\n",
    "‚è≥ 15 Minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa64954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bitte diese Codezelle ausf√ºhren und das Medium-Sprachmodell herunterladen\n",
    "!python -m spacy download de_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd55f909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ihre L√∂sung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf461c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da132e1b",
   "metadata": {},
   "source": [
    "### üìù **Jetzt:** Aufgabe 2 - NER auf einen Tagungsbericht anwenden\n",
    "\n",
    "- Wenden Sie das Sprachmodell `de_core_news_md` auf den Tagungsbericht an, mit dem wir bereits in der vorherigen Einheit zu Regul√§ren Ausdr√ºcken gearbeitet haben.\n",
    "- Visualisieren Sie erneut die Entit√§ten f√ºr den Text und speichern Sie die Visualisierung als HTML-Datei.\n",
    "- Vergleichen Sie die NER-Ergebnisse f√ºr die unterschiedlichen Textarten.\n",
    "\n",
    "‚è≥ 10 Minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a624f91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text = \"\"\"Der Arbeitskreis ‚ÄûDigital Humanities Munich‚Äú (dhmuc) widmete seinen ersten Workshop 2016 dem Thema ‚ÄûDigitale Daten in den Geisteswissenschaften. Interdisziplin√§re Perspektiven f√ºr semantische und strukturelle Analysen‚Äú. Insgesamt 14 Vortr√§ge er√∂rterten aktuelle Forschungen und Infrastrukturen im Bereich der maschinellen Textanalyse.\n",
    "Folgende Institutionen zeichneten f√ºr die Organisation des Workshops verantwortlich: dhmuc ‚Äì Arbeitskreis ‚ÄúDigital Humanities Munich‚Äù, das Institut f√ºr Computerlinguistik Universit√§t Z√ºrich , das Historische Seminar der Ludwig-Maximilians-Universit√§t M√ºnchen , die IT-Gruppe Geisteswissenschaften (ITG) der Ludwig-Maximilians-Universit√§t M√ºnchen sowie die Bayerische Akademie der Wissenschaften .\n",
    "\n",
    "Einf√ºhrung\n",
    "ECKHART ARNOLD (M√ºnchen), MARK HENGERER (M√ºnchen), NOAH BUBENHOFER (Z√ºrich) und CHRISTIAN RIEPL (M√ºnchen) machten klar, dass das Vorhandensein grosser Mengen an digitalen Textkorpora den Geisteswissenschaften neue Zukunftsperspektiven er√∂ffnet, gleichzeitig aber auch neue Herausforderungen an die Disziplinen stellt. Der Br√ºckenschlag zwischen Philologie und Informationstechnologie erfordert von den Forschenden ein hohes Mass an Technikaffinit√§t. Das Verst√§ndnis von Text als Daten und dessen interaktive graphische Visualisierung ver√§ndert die hermeneutischen Herangehensweisen und Methoden.\n",
    "\n",
    "Panel 1: Korpora\n",
    "Im Er√∂ffnungsreferat stellte PETER MAKAROV (Z√ºrich) sein PhD Projekt zum Thema ‚ÄûTowards automated content event analysis: Mining for protest events‚Äú vor. Es ist Teil des POLCON Projekts unter Professor Hanspeter Kriesi. Das Projekt bewegt sich zwischen Politikwissenschaft und Computerlinguistik. Ziel ist, herauszufinden, inwiefern moderne Natural Language Processing Technik die thematische Extraktion von Daten aus dem Internet unterst√ºtzen kann. Konkret geht es in der Studie um die Extraktion von News-Texten im Zusammenhang mit √ñffentlichen Protesten.\n",
    "Methodisch setzt das Projekt auf Machine-Learning-Verfahren. Hierzu m√ºssen geeignete Klassifizierungsmodelle erstellt und Entwicklungszyklen implementiert werden, mit deren Hilfe Trainings-Korpora erstellt werden k√∂nnen. In einem iterativen Prozess werden diese Trainingsdaten annotiert und verbessert. Dabei hat sich gezeigt, dass linguistische Standardmodelle zur Annotation nicht geeignet waren, um die Identifikation von Textstellen, die Protest indizierten, zu verbessern. Daher werden vereinfachte, besser auf die konkrete Fragestellung zugeschnittene Annotationssysteme entwickelt.\n",
    "DANIEL KNUCHEL (Z√ºrich) referierte unter dem Titel ‚ÄûHIV/AIDS diskurslinguistisch ‚Äì ein multimediales Korpus‚Äú √ºber sein Promotionsprojekt. In diesem analysiert er, welches Wissen heute zum Thema HIV/AIDS in der √ñffentlichkeit zirkuliert. Dazu baute der Referent ein Korpus aus unterschiedlichen Quellen (Massenmedien, Selbsthilfe-Blogs, Social Media) auf. Er wies darauf hin, dass bei solchen Vorhaben nebst den konzeptuellen und technischen Fragen die rechtlichen Bedingungen zur Datennutzung fr√ºhzeitig gekl√§rt werden m√ºssen. Wichtig sei zudem, dass von Anfang an auf Nachhaltigkeit geachtet werde, um sp√§tere Nutzungen der Daten zu erm√∂glichen.\n",
    "MAX HADERSBECK (M√ºnchen) berichtete in seinem Vortrag √ºber die Erfahrungen mit der FinderApp ‚ÄúWittFind‚Äú. Die webbasierte Applikation steht Forschenden nun seit vier Jahren zur Verf√ºgung, um den Open Access zug√§nglichen Teil des Nachlasses von Ludwig Wittgenstein nach W√∂rtern, Phrasen, S√§tzen und semantischen Begriffen zu durchsuchen. Sie setzt dazu regelbasierte computerlinguistische Verfahren ein. WittFind zeichnet sich dadurch aus, dass die gefundenen Textstellen zusammen mit dem Faksimileextrakt dargestellt werden und eine √úberpr√ºfung anhand des Originaltexts jederzeit m√∂glich ist.\n",
    "Unter anderem am Beispiel des englischen Neologismus ‚Äúcherpumple‚Äù stellte SUSANNE GRANDMONTAGNE (M√ºnchen), das System ‚ÄûNeoCrawler‚Äú vor. NeoCrawler verfolgt die Entstehung und Verbreitung von Neologismen im Internet. Die Ergebnisse werden automatisiert f√ºr weitere linguistische Analysen aufbereitet. Zudem stellt das System Zeitreihenverlaufsanalysen zur Verf√ºgung. Eine Benutzeroberfl√§che unterst√ºtzt die manuelle Kategorisierung der erhobenen Daten. Aus rechtlichen Gr√ºnden kann diese M√∂glichkeit jedoch nicht im Rahmen von Crowd Sourcing genutzt werden.\n",
    "Die prosopographische Erforschung der Herrschaftselite der Habsburgermonarchie steht im Zentrum des Projekts ‚ÄûKaiserhof‚Äú , das MARK HENGERER und GERHARD SCH√ñN (M√ºnchen) vorstellten. Nebst der eindeutigen Identifikation von Personen ist die Messbarkeit qualitativer Aspekte eine der haupts√§chlichen Herausforderungen, um Netzwerke und ‚ÄûReichweiten von Integration‚Äú visualisieren zu k√∂nnen. Dabei hat sich der Ansatz bew√§hrt, von Begriffen mittlerer Reichweite auszugehen. Visualisierungen (etwa von Verwandtschaftsbeziehungen oder Geolokalisierungen) sind in diesem Zusammenhang von hohem heuristischem Wert.\n",
    "\n",
    "Natural Language Processing / Suche \n",
    "Zeitangaben sind eine zentrale Information in historischen Dokumenten. Das war der Ausgangspunkt f√ºr die Pr√§sentation von NATALIA KORCHAGINA (Z√ºrich) zu ‚ÄúNatural language processing for historical documents‚Äú. Doch die maschinelle Textextraktion aus den oft handschriftlichen Dokumenten ist komplex. Ziel des Forschungsvorhabens der Referentin ist es, ein Tool f√ºr die automatisierte Extraktion von Zeitangaben aus historischen Texten zu entwickeln. Als Quellengrundlage dient ein Korpus von digitalisierten Schweizer Rechtstexten zwischen dem 10. und 18. Jahrhundert.\n",
    "In einer ersten Phase des Projekts wurde unter Nutzung des an der Universit√§t Heidelberg entwickelten Zeittaggers HeidelTime ein fehlerfreies, aber kleines manuell annotiertes Gold Standard Korpus erstellt. Auf dieser Grundlage wird sodann mit einem hybriden Vorgehen, das machine-learning und regelbasierte Methoden kombiniert, ein gr√∂sseres Silver Standard Korpus erarbeitet, das f√ºr die Extraktion von Zeitangaben herangezogen werden kann.\n",
    "Zeitgen√∂ssische Rechtstexte standen im Zentrum des Forschungsprojekts von KYOKO SUGISAKI (Z√ºrich). Sie pr√§sentierte in ihrem Vortrag unter dem Titel ‚ÄúNatural language processing in speziellen Textsorten, z.B. legislative Texte‚Äú ihre soeben abgeschlossene Doktorarbeit. Am Beispiel von online verf√ºgbaren Schweizer Gesetzestexten erstellte sie ein qualitativ hochwertiges Korpus von fachspezifischen Texten. Im Verlauf der Arbeiten zeigte sich, dass vorhandene Referenzkorpora (etwa die Sammlung Schweizerischer Rechtsquellen) genutzt werden k√∂nnen, jedoch an die Spezifika des Vorhabens angepasst werden m√ºssen. Mittels Kombination von verschiedenen Analyseverfahren und Tools (u.a. POS-tagging, morphosyntaktische Analyse, Style-Checking) konnte die Qualit√§t der Texterkennung deutlich verbessert werden.\n",
    "\n",
    "Visualisierung\n",
    "Unter dem Titel ‚ÄûVisualisierungen in den Digital Humanities‚Äú diskutierten NOAH BUBENHOFER, KLAUS ROTHENH√ÑUSLER und DANICA PAJOVIC (alle Z√ºrich) die theoretischen Grundlagen von Visualisierungen und hinterfragten g√§ngige Visualisierungspraktiken in den Digital Humanities. Ausgangspunkt ist die Feststellung, dass Visualisierungen nicht nur bei der Darstellung von Analyseergebnissen, sondern auch bei der Datenexploration eine wichtige Rolle spielen.\n",
    "Besonders bei explorativen Visualisierungen sind gem√§ss den Referenten methodisch-technische Aspekte wichtig. Denn Diagramme sind nicht Bilder. Sie sind hoch abstrahierte Darstellungen, die Hypothesen √ºber Sachverhalte transportieren. Visualisierungen lassen sich entlang einer Reihe von grafischen, datentypischen, diagrammatischen, semiotischen, √§sthetischen, technischen und diskursiven Attributen kategorisieren und beurteilen.\n",
    "Anhand der Darstellung von Geokollokationen, das hei√üt von sprachlichen √Ñu√üerungen √ºber Orte zeigte Noah Bubenhofer, wie durch Sprechen eine Vorstellung von Welt konstruiert wird. Die naheliegende Visualisierung von Geokollokationen auf einer Weltkarte ist dann ein voraussetzungsvoller Vorgang, der unhinterfragte Pr√§missen von Kartendarstellungen √ºbernimmt. Bubenhofer pl√§dierte daher daf√ºr, auch nicht kartenbasierte Visualisierungen in Betracht zu ziehen. Das Beispiel illustrierte, wie Denkstile, Software und technische M√∂glichkeiten in Visualisierungen mit einfliessen und diese in gewisser Weise vorbestimmen.\n",
    "MATTHIAS REINERT (M√ºnchen) pr√§sentierte in seinem Referat ‚Äûdeutsche-biographie.de ‚Äì ein historisch biografisches Informationssystem. Computerlinguistischer Ansatz und Visualisierung‚Äú, das aus diesem Vorhaben resultierende Internetangebot. In rund 48.000 Lexikonartikeln bietet es Informationen zu rund 540.000 Personen. F√ºr die zuverl√§ssige Volltexterfassung und -kodierung sowie den Normdatenabgleich von Personen und Orten wurden seit 2012 computerlinguistische Verfahren eingesetzt. Hierzu wurden lokale Grammatiken und eine korpora-spezifische Wortdatenbank erstellt. Das historisch-biografische Informationssystem erm√∂glicht eine Geo-Visualisierung und die Darstellung von Ego-Netzwerken. In der Diskussion betonte der Referent, dass im Zusammenhang mit einem solchen interaktiven Angebot eine transparente Kommunikation √ºber die M√∂glichkeiten und Grenzen des Systems und der Datenbasis unerl√§sslich ist, um den Nutzern die Einsch√§tzung der Evidenz der gewonnen Resultate zu erm√∂glichen.\n",
    "Unter dem Titel ‚ÄûTheatrescapes‚Äú argumentierte TOBIAS ENGLMEIER (M√ºnchen), dass die stetig wachsenden, nun auch f√ºr die Geisteswissenschaften verf√ºgbaren Datenbest√§nde oft nur mit Techniken der Informationsvisualisierung erfasst werden k√∂nnten und interpretierbar seien. Das Projekt ‚ÄûTheatrescapes. Mapping Global Theatre Histories‚Äú nutzt f√ºr die interaktive Kartendarstellungen WebGL (Web Graphics Library) und den Google Maps API . Der Referent zeigte auf, dass mittels dieses pragmatischen Ansatzes die technischen H√ºrden bei der Georeferenzierung von grossen Datenbest√§nden mit vertretbarem Aufwand √ºberwunden und ansprechende Resultate wie etwa interaktive historisierte Kartendarstellungen erzielt werden k√∂nnen. Allerdings betonte er, wie schon Noah Bubenhofer vor ihm, dass Entscheidungen √ºber den Einsatz von bestimmter Software √ºber technische Aspekte hinausreichen und auch inhaltliche Konsequenzen haben.\n",
    "EMMA MAGES (M√ºnchen) stellte den ‚ÄûAudioatlas Siebenb√ºrgisch-S√§chsischer Dialekte‚Äú (ASD) vor, einen interaktiven Online-Atlas. Er erschlie√üt eine umfangreiche Audiodokumentation deutscher Ortsdialekte Siebenb√ºrgens und der Marmarosch und macht diese in Transkription und Audioaufnahmen zug√§nglich. Nebst der eigentlichen Transkription wurde eine morphosyntaktische Etikettierung vorgenommen und eine Ontologie f√ºr die Erschliessung entworfen. Mittels Kartierung erlaubt der ASD unter anderem qualitative und quantitative Sichten auf die √∂rtliche Verteilung der Dialekte.\n",
    "In seinem Referat √ºber die Online-Plattform ‚ÄûVerbaAlpina‚Äú berichtete STEPHAN L√úCKE (M√ºnchen) √ºber die Herausforderungen, die sich bei diesem politische Grenzen √ºberschreitenden und mehrsprachigen Ansatz stellten. Ziel des Langzeitprojekts ‚ÄûVerba Alpina‚Äú ist es, den sprachlich stark fragmentierten Alpenraum in seiner kultur- und sprachgeschichtlichen Zusammengeh√∂rigkeit zu erschliessen. Das Projekt fokussiert auf die Wechselbeziehung (sowohl in onomasiologischer wie semasiologischer Perspektive) zwischen W√∂rtern und bezeichneten Konzepten. Die sprachlichen Zusammenh√§nge werden erg√§nzt mit ethnographischen, historischen und politischen Aspekten und in einer interaktiven Online-Karte mit Crowd-Sourcing-Komponente dargestellt.\n",
    "\n",
    "Crowd\n",
    "Das Referat ‚ÄûText+Berg ‚Äì 150 Jahre alpinistische Texte: OCR-Fehler, Crowd Correction‚Äú von SIMON CLEMATIDE (Z√ºrich) diskutierte die Voraussetzungen f√ºr erfolgreiches Crowd Sourcing. Im Rahmen des Projekts Text+Berg realisierte das Institut f√ºr Computerlinguistik der Universit√§t Z√ºrich eine Online-Plattform zur Korrektur des OCR-Textes der digitalisierten Jahrb√ºcher des Schweizerischen Alpenklubs (SAC) von 1864 bis 1899. Entscheidend f√ºr die Motivierung von Freiwilligen und damit f√ºr den Erfolg des Vorhabens waren eine sorgf√§ltig programmierte Benutzeroberfl√§che und ein einfacher Workflow. Dazu kamen begleitende Massnahmen, um die Motivation der Teilnehmenden aufrechtzuerhalten. Hierzu geh√∂rten spielerische Elemente und Layout-Massnahmen, die den Teilnehmenden R√ºckmeldungen zum Datenzustand und zur geleisteten Arbeit geben. Ein Vorteil war, dass es sich beim potentiellen Teilnehmerkreis um gut organisierte und am Thema interessierte Vereinsmitglieder mit hoher Sachkenntnis handelte.\n",
    "Abschliessend stellte GERHARD SCH√ñN (M√ºnchen) das Projek ‚ÄûPlay4Science‚Äú und die in diesem Rahmen entwickelte Spiel-Plattform ‚ÄûArtigo‚Äú vor. Das Projekt bringt Geisteswissenschaftler/innen, Informatiker/innen und Computerlinguist/innen zusammen, um zweckgerichtete soziale Spiel-Software (‚ÄûGames with a Purpose‚Äú (GWAP)) zu entwickeln, die seit einiger Zeit auch im wissenschaftlichen Bereich erfolgreich Crowd-Sourcing-Ans√§tze unterst√ºtzen. Ziel von Play4Science ist, eine anpassbare universelle Plattform anzubieten, die von allen F√§chern f√ºr verschiedenste Anwendungen sozialer Software genutzt werden kann.\n",
    "Die bereits realisierte Spiel-Plattform ‚ÄûArtigo‚Äú l√§dt zur Verschlagwortung von Gem√§lden aus einer Bilddatenbank ein. Sie schaltet zwei Mitspieler zusammen, die unabh√§ngig voneinander relevant erscheinende Begriffe f√ºr dieselben Bilder eingeben. Die solcherart Peer-validierten Begriffe werden in der Datenbank gespeichert und sind f√ºr sp√§tere Suchabfragen nutzbar.\n",
    "\n",
    "Fazit\n",
    "Der Workshop bot einen guten √úberblick √ºber den State-of-the-Art computerlinguistischer Ans√§tze f√ºr die digitale Aufbereitung von Textkorpora. Er wies auf die Herausforderungen hin, die sich bei der interdisziplin√§ren Zusammenarbeit an der Schnittstelle zwischen Technik und geisteswissenschaflticher Forschung stellen. Es wurde klar, dass eine fundierte Fragestellung entscheidend f√ºr den Erfolg von computerlinguistischen Vorhaben ist. Zugleich wurde aber unter dem Stichwort ‚Äûcode matters‚Äú auch betont, dass technologische Aspekte nicht vernachl√§ssigt werden d√ºrfen, da sie Einfluss auf Vorgehensweisen und Resultate haben. Von den Geisteswissenschaftlern muss daher verlangt werden, dass sie wissen, was die verwendeten Algorithmen tun. Dies gilt insbesondere auch f√ºr heuristisch und explorativ eingesetzte Visualisierungen, bei denen sich die Forschenden stets zu fragen haben, ob sie den generierten Visualisierungen genug kritisch gegen√ºberstehen. Unter geisteswissenschaftlichen Vorzeichen kann eine in den Visual Analytics mitunter unterstellte korrelationsbasierte ‚Äûground truth‚Äú nicht vorausgesetzt werden.\n",
    "In den Diskussionen hat sich ferner die Sicherstellung der Nachhaltigkeit in computerlinguistischen Vorhaben als zentraler Aspekt herausgestellt. Dabei geht es um mehr als Datenverf√ºgbarkeit. Entscheidend sind das Bewusstsein f√ºr die Br√ºchigkeit der Datengrundlagen und der Umgang mit Unsch√§rfen und Unvollkommenheiten. In einer weiteren Perspektive identifizierte der Workshop eine Reihe zentraler Erfolgsfaktoren f√ºr Digital Humanities-Projekte. So gilt es die rechtlichen Bedingungen f√ºr die Datennutzung fr√ºhzeitig zu kl√§ren, eine ausbauf√§hige Infrastruktur aufzubauen, die Mitarbeitenden auszuw√§hlen, auszubilden und zu begeistern sowie die langfristige Finanzierung sicherzustellen. Insgesamt bot die Tagung einen guten √úberblick √ºber die M√∂glichkeiten der maschinellen Analyse und Interpretation von Texten. Es herrschte Konsens dar√ºber, dass sich die Geisteswissenschaften dadurch in den kommenden Jahren stark ver√§ndern werden.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a1a3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ihre L√∂sung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51595fd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a56b50",
   "metadata": {},
   "source": [
    "### üìù **Jetzt:** Aufgabe 3 - NER auf einen Wikipedia-Text anwenden\n",
    "- Wenden Sie das Sprachmodell `de_core_news_md` auf den Wikipedia-Eintrag zu [\"Hiernymus Bosch\"](https://de.wikipedia.org/wiki/Hieronymus_Bosch) an. Hintergrund: Das Sprachmodell wurde u.a. auf Wiki-Daten trainiert.\n",
    "- Visualisieren Sie erneut die Entit√§ten f√ºr den Text und speichern Sie die Visualisierung als HTML-Datei.\n",
    "- Vergleichen Sie die NER-Ergebnisse f√ºr die unterschiedlichen Textarten.\n",
    "\n",
    "‚è≥ 5 Minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f8735680",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_text = \"\"\"\n",
    "Hieronymus Bosch […¶ijeÀêÀà…æoÀênim ès Ààb…îs] (* um 1450 in ‚Äôs-Hertogenbosch als Jheronimus van Aken [je…™Àà…æoÀênim ès v…ën ÀàaÀêk…ô(n)]; ‚Ä† August 1516 ebenda) war ein niederl√§ndischer Maler der Sp√§tgotik bzw. der Renaissance. Bosch stammte aus einer Malerfamilie. Er hatte seine Auftraggeber vielfach im h√∂heren Adel und Klerus. Seine Gem√§lde, meist in √ñl auf Eichenholz, zeigen in der Regel religi√∂se Motive und Themen. Sie sind reich an Figuren, Fabelwesen und ungew√∂hnlichen Bildelementen, deren Deutungszusammenhang und Interpretation oft nicht gesichert sind. Boschs Werk findet bis heute regelm√§√üige Aufmerksamkeit und wurde in der Kunst vielfach rezipiert. √úber sein Leben gibt es nur wenige gesicherte Anhaltspunkte.\n",
    "\n",
    "Herkunft und Name\n",
    "Hieronymus Bosch entstammte der Malerfamilie ‚Äûvan Aken‚Äú, deren Herkunftsname darauf verweist, dass die direkten Vorfahren in der v√§terlichen Linie aus Aachen stammten. Vier Generationen von Malern sind nachgewiesen: Hieronymus Boschs Urgro√üvater Thomas van Aken war als Maler in Nijmegen t√§tig. Sein Gro√üvater Jan van Aken zog um 1426 von Nijmegen in die aufstrebende Stadt ‚Äôs-Hertogenbosch. Vier der f√ºnf S√∂hne Jans, darunter Hieronymus‚Äô Vater Anthonius van Aken, wurden ebenfalls Maler. Seinen sozialen Aufstieg kr√∂nte Anthonius 1462 mit dem Erwerb eines steinernen Hauses direkt am Marktplatz, in das er auch seine Malerwerkstatt verlagerte. Mit seiner Gattin Aleid van der Mynnen hatte er f√ºnf Kinder: zwei T√∂chter (eine hie√ü Herberta) und die drei S√∂hne: Goeswinus oder Goessen van Aken, Jan van Aken und, als f√ºnftes Kind, Jheronimus van Aken (Hieronymus).\n",
    "Hieronymus benannte sich sp√§ter nach seiner Heimatstadt, die auch Den Bosch genannt wird. In Spanien, wo einige seiner bedeutendsten Gem√§lde im Museo del Prado ausgestellt sind, spricht man von El Bosco\n",
    "\n",
    "Leben\n",
    "Hieronymus folgte wie seine beiden √§lteren Br√ºder der Familientradition und erhielt wie sie seine Malerausbildung zumindest zeitweise in der v√§terlichen Werkstatt. Nach dem Tod des Vaters f√ºhrte Goessen als √§ltester Sohn die Werkstatt weiter.\n",
    "Hieronymus Bosch wurde erstmals 1474 urkundlich erw√§hnt. 1481 heiratete er die Patriziertochter Aleyt Goyaert van de Mervenne, die ein Haus und ein Landgut in die Ehe einbrachte. Das verhalf Bosch zu einer gr√∂√üeren Unabh√§ngigkeit.\n",
    "1488 trat er der religi√∂sen Bruderschaft Unserer Lieben Frau der √∂rtlichen St.-Johannes-Kathedrale bei, erst als √§u√üeres, dann als geschworenes Mitglied. Der elit√§re innere Zirkel der Bruderschaft umfasste etwa 60 Personen, die in der Regel aus der h√∂chsten aristokratischen beziehungsweise patrizischen st√§dtischen Schicht kamen und fast alle Geistliche verschiedener Weihegrade waren. Fast die H√§lfte waren weltliche Priester, die teilweise zugleich Notare waren. Ferner gab es unter ihnen √Ñrzte und Apotheker sowie einige wenige K√ºnstler: Musiker, einen Architekten und nur einen Maler ‚Äì Hieronymus Bosch. Die Bruderschaft pflegte Kontakt zu den h√∂chsten Kreisen des Adels, der Geistlichkeit und der st√§dtischen Eliten in den Niederlanden. Neben dieser politisch-gesellschaftlichen Seite war sie gleicherma√üen religi√∂s ausgerichtet und wurde von den Dominikanern betreut. Die Mitglieder trafen sich einmal im Monat zum Mahl und zweimal in der Woche zur Messe. Johannes-, Marien- und andere Festtage wurden durch geistliche Spiele und Prozessionen begangen. In den Reihen der Br√ºder und vermittelt durch deren Kontakte zum Hochadel fand Bosch seine Auftraggeber.[1]\n",
    "Neben der Liebfrauenbruderschaft arbeitete er f√ºr die st√§dtische Elite und den niederl√§ndischen Hochadel. Zu seinen bedeutendsten Auftraggebern geh√∂rten der regierende F√ºrst der Niederlande Erzherzog Philipp der Sch√∂ne und sein Hof.\n",
    "Bosch starb 1516 im Alter von etwa 65 Jahren in seiner Heimatstadt. \n",
    "\n",
    "Werk\n",
    "Malweise, Themen\n",
    "Hieronymus Bosch lebte im Zeitalter der Renaissance, einer Periode des √∂konomischen Aufbruchs, der f√ºrstlichen Machtpolitik und der Forderung nach religi√∂ser und sittlicher Erneuerung. Er unterzog in seinen Bildern alle St√§nde einer Kritik, nicht nur den Klerus. Bosch malte vielfach religi√∂se Motive und Thematiken. Triptychen wie Der Heuwagen und Der Garten der L√ºste waren dagegen eindeutig nicht f√ºr einen Altar gedacht, sondern zur Beeindruckung und Unterhaltung eines h√∂fischen Publikums.\n",
    "Sein Werk entzieht sich einer einfachen Deutung: W√§hrend es teils plausible Interpretationen seiner Werke gibt, sind viele Darstellungen r√§tselhaft geblieben bzw. die Interpretation strittig. Bosch selbst hat keine schriftlichen Aufzeichnungen zu seinen Werken hinterlassen.\n",
    "Bosch malte zumeist mit √ñlfarben, selten mit Tempera, auf Eichenholz. Seine Palette war in vielen Bildern nicht sehr reichhaltig.[2] Er setzte Azurit f√ºr den Himmel und Landschaften im Hintergrund, gr√ºne Lasuren und kupferhaltige Pigmente (Malachit und Kupfer(II)-acetat (Gr√ºnspan)) f√ºr Laub und Landschaften im Vordergrund und Bleizinngelb, Ocker und Roten Lack (Karmin oder F√§rberkrapp) f√ºr die wichtigen Bildelemente ein.[3]\n",
    "Erhalten geblieben sind von Boschs Werken vor allem Gem√§lde auf Holztafeln, daneben einige Zeichnungen auf Papier.\n",
    "\n",
    "Zuschreibungen\n",
    "In der Vergangenheit wurden Gem√§lde einerseits neu Hieronymus Bosch zugeschrieben, andere aufgrund neuerer Erkenntnisse ihm nicht mehr zugeordnet. Bei diesen Neubewertungen der einschl√§gigen Werke waren die Einsch√§tzungen oft strittig.\n",
    "In der Vorbereitung einer Bosch-Ausstellung in Rotterdam 2001 untersuchte Peter Klein von der Universit√§t Hamburg die von Bosch und seiner Werkstatt als Maluntergrund benutzten Eichentafeln mit der Analysemethode der Dendrochronologie. In Folge mussten einige bislang Bosch zugeschriebene Werke aus dem Gesamtwerk ausgeschieden werden, so Die Hochzeit zu Kana. Die Tafeln bestehen aus Holz von B√§umen, die zum Teil Jahrzehnte nach Boschs Tod gef√§llt worden sein sollen. Kleins Bewertungen werden allerdings kontrovers diskutiert.[4]\n",
    "Im Rahmen der Vorbereitung zur gro√üen Bosch-Jubil√§umsausstellung anl√§sslich seines 500. Todestages im Fr√ºhjahr 2016 im Noordbrabants Museum in ‚Äôs-Hertogenbosch wurden durch das Bosch Research and Conservation Project (BRCP) 21 Gem√§lde und 20 Zeichnungen als eigenh√§ndig bewertet.[5] Nach dem catalogue raisonn√© von 2016 werden Der Gaukler (Saint-Germain-en-Laye, Mus√©e municipal), Das Steinschneiden und Die sieben Tods√ºnden nicht mehr Bosch zugeschrieben. \n",
    "\n",
    "[Auszug Ende]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c878ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ihre L√∂sung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "54ee2d97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0276d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
