{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíª Einf√ºhrung in Topic Modeling mit Python - Step 1: Textdaten vorbereiten\n",
    "\n",
    "## 1. Sortiertes Datenkorpus laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read csv into dataframe object\n",
    "corpus = pd.read_csv(\"../daten/speeches-bundesregierung_sorted.csv\", encoding=\"utf-8\") \n",
    "print(f\"Der Datzensatz enth√§lt {corpus.shape[0]} Reden mit {corpus.shape[1]} Attributen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the speeches from the column text and store the\n",
    "# texts as elements in a python-list-object\n",
    "list_of_speeches = corpus.loc[:, \"text\"].tolist()\n",
    "print(list_of_speeches[-3][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute token count\n",
    "raw_token_count = 0\n",
    "for speech in list_of_speeches:\n",
    "    raw_token_count += len(speech.split(\" \"))\n",
    "\n",
    "print(f\"Das einfach tokenisierte Korpus auf Basis von Whitespaces enth√§lt {raw_token_count} Token.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## √úberarbeitung der Textdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from gensim.utils import tokenize, simple_preprocess\n",
    "\n",
    "# optional: removing stopwords - for example via nltk\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "# import NLTK stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(speeches):\n",
    "    \n",
    "    \"\"\"Turns a list of strings into a tokenized list of words.\"\"\"\n",
    "    \n",
    "    tokenized = []\n",
    "    \n",
    "    for speech in speeches:\n",
    "        # gensims tokenize-function by default only tokenizes, returns alphabetic characters\n",
    "        # note: no digits or punctuation are yielded\n",
    "        tokenized.append(list(tokenize(speech)))\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "speeches_tokenized = sent_to_words(list_of_speeches) \n",
    "\n",
    "print(speeches_tokenized[-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisierung und POS-Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncomment in binder to download language model\n",
    "\n",
    "#!python -m spacy download de_core_news_sm # small language model\n",
    "#!python -m spacy download de_core_news_lg # large language model\n",
    "\n",
    "!python -m spacy download de_core_news_md # medium language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import de_core_news_md\n",
    "nlp = de_core_news_md.load(disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatize and filter by pos-tag\n",
    "# note: this may take some time in Binder\n",
    "\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"PROPN\", 'ADJ', 'VERB']):\n",
    "    \"\"\"Takes a list of tokenized texts and only returns lemmatized data\n",
    "    depending on allowed POS tags\"\"\"\n",
    "    texts_out = []\n",
    "    #counter = 0\n",
    "    for text in texts:\n",
    "        doc = nlp(\" \".join(text)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags]) # list comprehension\n",
    "        #counter += 1 # optional to show progress in output\n",
    "        #print(f\"Text {counter} lemmatisiert.\")\n",
    "    return texts_out\n",
    "\n",
    "data_lemmatized = lemmatization(speeches_tokenized, allowed_postags=['NOUN', 'ADJ', 'VERB', \"PROPN\"])\n",
    "\n",
    "print(data_lemmatized[-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stoppw√∂rter entfernen (optional) und normalisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get typical german stopwords\n",
    "stop_words = stopwords.words(\"german\")\n",
    "\n",
    "# optional: extend nltk stopwort list\n",
    "#stop_words.extend([\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopword-removal and normalization\n",
    "def remove_stopwords(texts):\n",
    "    \n",
    "    \"\"\"\"Takes list of tokenized texts, filters stopwords \n",
    "    and returns normalized list of words (all lowercase, min 2 character tokens\n",
    "    and max 100 character tokens).\"\"\"\n",
    "    \n",
    "    preprocessed_texts = []\n",
    "    \n",
    "    for text in texts:\n",
    "        word_list = []\n",
    "        \n",
    "        for word in simple_preprocess(str(text), min_len=2, max_len=100):\n",
    "            if word not in stop_words:\n",
    "                word_list.append(word)\n",
    "                \n",
    "        preprocessed_texts.append(word_list)\n",
    "            \n",
    "    return preprocessed_texts\n",
    "\n",
    "data_words_nostops = remove_stopwords(data_lemmatized)\n",
    "\n",
    "print(data_words_nostops[-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üìù **Jetzt:** Aufgabe - Tokencount nach dem Preprocessing \n",
    "Z√§hlen Sie in Anlehnung an den obigen Codeblock die Anzahl der Token nach dem Preprocessing. Wie hat sich die Datengrundlage ver√§ndert?\n",
    "\n",
    "‚è≥ 5 Minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorverarbeitete Textdaten in den Dataframe speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speeches_string = []\n",
    "\n",
    "for speech in data_words_nostops:\n",
    "    speeches_string.append(\" \".join(speech[:]))\n",
    "\n",
    "# save normalized data to dataframe\n",
    "corpus[\"preprocessed_text\"] = speeches_string\n",
    "corpus[\"preprocessed_text\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data for topic modeling\n",
    "corpus.to_csv(\"../daten/speeches-bundesregierung_preprocessed.csv\", index=False, encoding=\"utf-8\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab2bc7d93ca595a8547f73d141c9dac102da280dfea0255e9d7a3267ba9f4e63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
