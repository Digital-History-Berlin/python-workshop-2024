{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“ Ãœbung zum Data Mining mit Pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2: Anzahl der Token pro Rede ermitteln und speichern\n",
    "\n",
    "Wir wollen die Anzahl der Token, also die LÃ¤nge der einzelnen Reden ermitteln und die Informationen wieder im Dataframe als neue Spalte speichern.\n",
    "\n",
    "- Lesen Sie dazu die Texte aus der Spalte `text` aus und speichern Sie sie in einer Liste. \n",
    "- Sorgen Sie nun dafÃ¼r, dass die einzelnen Texte in Wortlisten zerlegt werden und bestimmen Sie die LÃ¤nge der einzelnen Wortlisten. \n",
    "- Die LÃ¤nge der Texte soll als neue Spalte `ntokens` zum Dataframe hinzugefÃ¼gt werden.\n",
    "\n",
    "â³ 10 Minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle bitte ausfÃ¼hren: Pandas importieren und Dataframe einlesen\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"../daten/speeches-bundesregierung.csv\", parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.loc[:, \"text\"].to_list()         # Texte in Liste einlesen\n",
    "\n",
    "ntokens = []                                \n",
    "for text in texts:                          # Die einzelnen Texten per Schleife durchgehen\n",
    "    ntokens.append(len(text.split()))       # jeden Text mit der Funktion split() in eine Liste zerlegen und die LÃ¤nge der Liste ermitteln und zur Liste ntokens hinzufÃ¼gen\n",
    "\n",
    "df.loc[:, \"ntokens\"] = ntokens              # fertige Liste zum Dataframe hinzufÃ¼gen\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 3: Datenabfragen gestalten\n",
    "\n",
    "Erkunden Sie das Korpus von Reden von AngehÃ¶rigen der Bundesregierung, um ein tieferes VerstÃ¤ndnis verschiedener thematischer Schwerpunkte und die Nutzung spezifischer Begrifflichkeiten Ã¼ber die Zeit zu gewinnen. \n",
    "\n",
    "1. Extrahieren Sie alle Reden, die im Jahr 2001 gehalten wurden.\n",
    "2. Ermitteln Sie die Anzahl der Reden, die das Wort \"Europa\" enthalten.\n",
    "3. Identifizieren Sie Reden, die den Begriff \"digital\" und \"Digitalisierung\" enthalten. \n",
    "4. Suchen Sie nach Reden, die zwischen 2000 und 2010 gehalten wurden, die sich auf Umweltthemen beziehen. In Frage kommen hierfÃ¼r Begriffe wie \"Umwelt\", \"Klima\", \"Nachhaltigkeit\" oder Ã¤hnliches. Formulieren Sie eine Abfrage, die mehrere dieser Begriffe berÃ¼cksichtigt. Achten Sie ggf. darauf, wie die Bedingungen mit runden Klammern gruppiert werden mÃ¼ssen.\n",
    "\n",
    "Hinweise:\n",
    "- FÃ¼r diese Aufgabe benÃ¶tigen Sie die logischen Operatoren fÃ¼r *UND* (`&`) und *ODER* (`|`) - letzteres ist je nach Tastatur auf einer anderen Taste. Wichtig: In Python hat der logische Operator `&` eine hÃ¶here PrioritÃ¤t als `|`. Bei Abfragen, die beide Operatoren verwenden, muss also darauf geachtet werden, wie die Abfragebestandteile mit runden Klammern gruppiert werden.\n",
    "- Die Funktion `contains()` arbeitet standardmÃ¤ÃŸig case-sensitiv. Wenn Sie eine case-insensitive Suche durchfÃ¼hren mÃ¶chten, kÃ¶nnen Sie den Parameter `case=False` ergÃ¤nzen.\n",
    "\n",
    "â³ 20 Minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zelle bitte ausfÃ¼hren: Pandas importieren und Dataframe einlesen\n",
    "import pandas as pd\n",
    "df = pd.read_json(\"../daten/output/speeches-bundesregierung_bearbeitet.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erste Aufgabe\n",
    "mask = df.loc[:, \"date\"].dt.year == 2001\n",
    "year_2001 = df.loc[mask, [\"date\", \"text\"]]\n",
    "year_2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zweite Aufgabe\n",
    "mask = df.loc[:, \"text\"].str.contains(\"Europa\").sum()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dritte Aufgabe\n",
    "mask = (df.loc[:, \"text\"].str.contains(\"digital\", case=False)) \\\n",
    "        & (df.loc[:, \"text\"].str.contains(\"Digitalisierung\", case=False))\n",
    "\n",
    "df_digital = df.loc[mask, :]\n",
    "print(df_digital.shape)\n",
    "df_digital.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context(\"display.max_colwidth\", None):\n",
    "    display(df_digital.loc[:, [\"date\", \"person\", \"text\"]].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vierte Aufgabe; Oder-Abfragen sind durch runde Klammern als eine Gruppe zusammengefasst\n",
    "\n",
    "mask = ((df.loc[:, \"text\"].str.contains(\"Umwelt\", case=False)) \\                \n",
    "        | (df.loc[:, \"text\"].str.contains(\"Klima\", case=False)) \\\n",
    "        | (df.loc[:, \"text\"].str.contains(\"Nachhaltigkeit\", case=False))) \\\n",
    "        & (df.loc[:, \"date\"].dt.year >= 2000) \\\n",
    "        & (df.loc[:, \"date\"].dt.year <= 2010) \n",
    "\n",
    "df_umwelt = df.loc[mask, :]\n",
    "print(df_umwelt.shape)\n",
    "df_umwelt.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mininghistoriansweb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
